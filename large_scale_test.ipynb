{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "224e50e9",
   "metadata": {},
   "source": [
    "# S3 Vectors Large-Scale Test\n",
    "\n",
    "Comprehensive testing of S3 Vectors with large datasets and IVFPQ indexing.\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Large-scale vector operations (50k+ vectors)\n",
    "- IVFPQ index creation and optimization\n",
    "- Performance testing with real-world datasets\n",
    "- Batch processing and efficient data handling\n",
    "\n",
    "**Note**: This notebook requires significant compute time and memory for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1028e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and check existing buckets\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('/home/rajan/Desktop/work/genai-vectors-py/src')\n",
    "\n",
    "# Connect to S3 Vectors\n",
    "from app.s3vectors_client import create_s3vectors_client\n",
    "\n",
    "s3vectors_client = create_s3vectors_client(\n",
    "    endpoint_url='http://localhost:8000',\n",
    "    aws_access_key_id='minioadmin',\n",
    "    aws_secret_access_key='minioadmin123',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "print(\"üîç Checking existing buckets and their vector counts...\")\n",
    "\n",
    "# List all buckets\n",
    "try:\n",
    "    buckets_response = s3vectors_client.list_vector_buckets()\n",
    "    buckets = buckets_response.get('vectorBuckets', [])\n",
    "    print(f\"Found {len(buckets)} existing buckets:\")\n",
    "    \n",
    "    for bucket in buckets:\n",
    "        bucket_name = bucket['vectorBucketName']\n",
    "        print(f\"\\nüì¶ Bucket: {bucket_name}\")\n",
    "        \n",
    "        # List indexes in this bucket\n",
    "        try:\n",
    "            indexes_response = s3vectors_client.list_indexes(vectorBucketName=bucket_name)\n",
    "            indexes = indexes_response.get('indexes', [])\n",
    "            \n",
    "            if indexes:\n",
    "                for index in indexes:\n",
    "                    index_name = index['indexName']\n",
    "                    print(f\"  üìä Index: {index_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error listing indexes: {e}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error listing buckets: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123177f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding function using local embedding service\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"Generate text embedding using local embedding service (LM Studio or Ollama).\"\"\"\n",
    "    try:\n",
    "        # Try LM Studio first (common local embedding service)\n",
    "        response = requests.post(\n",
    "            \"http://localhost:1234/v1/embeddings\",\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            json={\n",
    "                \"input\": text,\n",
    "                \"model\": \"text-embedding-nomic-embed-text-v1.5\"  # Common embedding model\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            embedding = data['data'][0]['embedding']\n",
    "            return embedding\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è LM Studio returned status {response.status_code}, trying Ollama...\")\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ö†Ô∏è LM Studio not available, trying Ollama...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è LM Studio error: {e}, trying Ollama...\")\n",
    "    \n",
    "    try:\n",
    "        # Try Ollama as fallback\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/embeddings\",\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            json={\n",
    "                \"model\": \"nomic-embed-text\",\n",
    "                \"prompt\": text\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            embedding = data['embedding']\n",
    "            return embedding\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Ollama returned status {response.status_code}\")\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ö†Ô∏è Ollama not available either, using random embedding...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Ollama error: {e}, using random embedding...\")\n",
    "    \n",
    "    # Fallback to normalized random embedding\n",
    "    print(\"üîÑ Using normalized random embedding as final fallback\")\n",
    "    vector = np.random.rand(768) - 0.5  # Center around 0\n",
    "    # Normalize to unit vector\n",
    "    norm = np.linalg.norm(vector)\n",
    "    if norm > 0:\n",
    "        vector = vector / norm\n",
    "    return vector.tolist()\n",
    "\n",
    "# Test the embedding function\n",
    "test_embedding = get_text_embedding(\"test query\")\n",
    "print(f\"‚úÖ Embedding function working, dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc26d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bucket for large-scale test (or reuse existing)\n",
    "bucket_name = \"large-scale-test-50k\"\n",
    "index_name = \"ivfpq-index\"\n",
    "\n",
    "print(f\"üöÄ Setting up large-scale test with {bucket_name}\")\n",
    "\n",
    "# Try to create bucket (will succeed if it doesn't exist)\n",
    "try:\n",
    "    response = s3vectors_client.create_vector_bucket(\n",
    "        vectorBucketName=bucket_name\n",
    "    )\n",
    "    print(f\"‚úÖ Created new bucket: {bucket_name}\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(f\"üì¶ Using existing bucket: {bucket_name}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error creating bucket: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7355470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and insert 10,000 vectors in batches (smaller for demo)\n",
    "print(\"üî¢ Generating and inserting 10,000 vectors...\")\n",
    "\n",
    "# Categories for diversity\n",
    "categories = [\"technology\", \"science\", \"history\", \"literature\", \"sports\", \"music\", \"art\", \"travel\", \"food\", \"nature\"]\n",
    "\n",
    "total_vectors = 10000  # Reduced for demo\n",
    "batch_size = 100\n",
    "vectors_inserted = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for batch_num in range(0, total_vectors, batch_size):\n",
    "    batch_vectors = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        if vectors_inserted >= total_vectors:\n",
    "            break\n",
    "            \n",
    "        doc_id = f\"doc_{vectors_inserted}\"\n",
    "        category = categories[vectors_inserted % len(categories)]\n",
    "        text = f\"This is document {vectors_inserted} about {category} with detailed content and information.\"\n",
    "        \n",
    "        # Get embedding\n",
    "        embedding = get_text_embedding(text)\n",
    "        \n",
    "        batch_vectors.append({\n",
    "            \"key\": doc_id,\n",
    "            \"data\": {\"float32\": embedding},\n",
    "            \"metadata\": {\n",
    "                \"text\": text,\n",
    "                \"category\": category,\n",
    "                \"doc_id\": doc_id\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        vectors_inserted += 1\n",
    "    \n",
    "    if batch_vectors:\n",
    "        # Insert batch\n",
    "        try:\n",
    "            s3vectors_client.put_vectors(\n",
    "                vectorBucketName=bucket_name,\n",
    "                indexName=index_name,\n",
    "                vectors=batch_vectors\n",
    "            )\n",
    "            \n",
    "            # Progress update every 1000 vectors\n",
    "            if vectors_inserted % 1000 == 0 or vectors_inserted == total_vectors:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = vectors_inserted / elapsed if elapsed > 0 else 0\n",
    "                print(f\"üìä Inserted {vectors_inserted:,}/{total_vectors:,} vectors ({rate:.1f} vectors/sec)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error inserting batch at {vectors_inserted}: {e}\")\n",
    "            break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Completed! Inserted {vectors_inserted:,} vectors in {total_time:.1f} seconds\")\n",
    "print(f\"üìà Average rate: {vectors_inserted/total_time:.1f} vectors/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e39f6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IVFPQ index for efficient similarity search\n",
    "print(f\"üîß Creating IVFPQ index '{index_name}' for large-scale search...\")\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = s3vectors_client.create_index(\n",
    "        vectorBucketName=bucket_name,\n",
    "        indexName=index_name,\n",
    "        dimension=768,\n",
    "        dataType=\"float32\",\n",
    "        distanceMetric=\"cosine\"\n",
    "    )\n",
    "    \n",
    "    index_time = time.time() - start_time\n",
    "    print(f\"‚úÖ IVFPQ index created successfully in {index_time:.1f} seconds!\")\n",
    "    print(f\"üìä Index details: {response}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(f\"üìä Index '{index_name}' already exists, using existing index\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error creating index: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e393558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search performance\n",
    "print(\"üîç Testing semantic search performance with large dataset...\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What is artificial intelligence and machine learning?\",\n",
    "    \"How do neural networks work in deep learning?\",\n",
    "    \"Python programming language features and applications\",\n",
    "    \"Vector search and similarity algorithms in databases\"\n",
    "]\n",
    "\n",
    "# Warm up the index\n",
    "print(\"üî• Warming up index with test query...\")\n",
    "warmup_embedding = get_text_embedding(\"warmup query\")\n",
    "try:\n",
    "    warmup_result = s3vectors_client.query_vectors(\n",
    "        vectorBucketName=bucket_name,\n",
    "        indexName=index_name,\n",
    "        queryVector={\"float32\": warmup_embedding},\n",
    "        topK=5,\n",
    "        returnMetadata=True\n",
    "    )\n",
    "    print(\"‚úÖ Index warmed up successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error warming up index: {e}\")\n",
    "\n",
    "# Test search performance\n",
    "print(\"\\nüèÉ Running performance tests...\")\n",
    "\n",
    "for i, query_text in enumerate(test_queries, 1):\n",
    "    print(f\"\\nQuery {i}: {query_text[:50]}...\")\n",
    "    \n",
    "    query_start = time.time()\n",
    "    query_embedding = get_text_embedding(query_text)\n",
    "    query_time = time.time() - query_start\n",
    "    \n",
    "    search_start = time.time()\n",
    "    try:\n",
    "        results = s3vectors_client.query_vectors(\n",
    "            vectorBucketName=bucket_name,\n",
    "            indexName=index_name,\n",
    "            queryVector={\"float32\": query_embedding},\n",
    "            topK=10,\n",
    "            returnMetadata=True\n",
    "        )\n",
    "        \n",
    "        search_time = time.time() - search_start\n",
    "        \n",
    "        vectors = results.get('vectors', [])\n",
    "        print(f\"  üìä Results: {len(vectors)} vectors found\")\n",
    "        print(f\"  ‚è±Ô∏è  Embedding time: {query_time*1000:.1f}ms\")\n",
    "        print(f\"  ‚è±Ô∏è  Search time: {search_time*1000:.1f}ms\")\n",
    "        print(f\"  ‚è±Ô∏è  Total time: {(query_time + search_time)*1000:.1f}ms\")\n",
    "        \n",
    "        # Show top results\n",
    "        for j, result in enumerate(vectors[:3], 1):\n",
    "            key = result.get('key', 'Unknown')\n",
    "            metadata = result.get('metadata', {})\n",
    "            distance = result.get('distance', 0.0)\n",
    "            similarity = 1 - distance\n",
    "            category = metadata.get('category', 'N/A')\n",
    "            print(f\"    {j}. {key} (similarity: {similarity:.3f}, category: {category})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        search_time = time.time() - search_start\n",
    "        print(f\"  ‚ùå Search failed after {search_time*1000:.1f}ms: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Large-scale performance tests completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1928643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metadata filtering at scale\n",
    "print(\"üîç Testing metadata filtering at scale...\")\n",
    "\n",
    "# Test query\n",
    "test_query = \"artificial intelligence and machine learning technologies\"\n",
    "query_embedding = get_text_embedding(test_query)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Filter by category = \"technology\"\n",
    "print(\"üìä Test 1: Filter by category = 'technology'\")\n",
    "filter_start = time.time()\n",
    "try:\n",
    "    results = s3vectors_client.query_vectors(\n",
    "        vectorBucketName=bucket_name,\n",
    "        indexName=index_name,\n",
    "        queryVector={\"float32\": query_embedding},\n",
    "        topK=10,\n",
    "        returnMetadata=True,\n",
    "        filter={\n",
    "            \"operator\": \"equals\",\n",
    "            \"metadata_key\": \"category\",\n",
    "            \"value\": \"technology\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    filter_time = time.time() - filter_start\n",
    "    vectors = results.get('vectors', [])\n",
    "    print(f\"  Found {len(vectors)} technology-related results in {filter_time*1000:.1f}ms:\")\n",
    "    for j, result in enumerate(vectors[:3], 1):\n",
    "        key = result.get('key', 'Unknown')\n",
    "        metadata = result.get('metadata', {})\n",
    "        distance = result.get('distance', 0.0)\n",
    "        similarity = 1 - distance\n",
    "        category = metadata.get('category', 'N/A')\n",
    "        print(f\"    {j}. {key} (similarity: {similarity:.3f}, category: {category})\")\n",
    "except Exception as e:\n",
    "    filter_time = time.time() - filter_start\n",
    "    print(f\"  ‚ùå Error in technology filter after {filter_time*1000:.1f}ms: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Test 2: Filter by category in ['science', 'technology']\n",
    "print(\"üìä Test 2: Filter by category in ['science', 'technology']\")\n",
    "filter_start = time.time()\n",
    "try:\n",
    "    results = s3vectors_client.query_vectors(\n",
    "        vectorBucketName=bucket_name,\n",
    "        indexName=index_name,\n",
    "        queryVector={\"float32\": query_embedding},\n",
    "        topK=10,\n",
    "        returnMetadata=True,\n",
    "        filter={\n",
    "            \"operator\": \"in\",\n",
    "            \"metadata_key\": \"category\",\n",
    "            \"value\": [\"science\", \"technology\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    filter_time = time.time() - filter_start\n",
    "    vectors = results.get('vectors', [])\n",
    "    print(f\"  Found {len(vectors)} science/technology results in {filter_time*1000:.1f}ms:\")\n",
    "    for j, result in enumerate(vectors[:3], 1):\n",
    "        key = result.get('key', 'Unknown')\n",
    "        metadata = result.get('metadata', {})\n",
    "        distance = result.get('distance', 0.0)\n",
    "        similarity = 1 - distance\n",
    "        category = metadata.get('category', 'N/A')\n",
    "        print(f\"    {j}. {key} (similarity: {similarity:.3f}, category: {category})\")\n",
    "except Exception as e:\n",
    "    filter_time = time.time() - filter_start\n",
    "    print(f\"  ‚ùå Error in science/technology filter after {filter_time*1000:.1f}ms: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Test 3: No filter (baseline)\n",
    "print(\"üìä Test 3: No filter (baseline comparison)\")\n",
    "baseline_start = time.time()\n",
    "try:\n",
    "    results = s3vectors_client.query_vectors(\n",
    "        vectorBucketName=bucket_name,\n",
    "        indexName=index_name,\n",
    "        queryVector={\"float32\": query_embedding},\n",
    "        topK=10,\n",
    "        returnMetadata=True\n",
    "    )\n",
    "    \n",
    "    baseline_time = time.time() - baseline_start\n",
    "    vectors = results.get('vectors', [])\n",
    "    print(f\"  Found {len(vectors)} total results in {baseline_time*1000:.1f}ms (baseline):\")\n",
    "    for j, result in enumerate(vectors[:3], 1):\n",
    "        key = result.get('key', 'Unknown')\n",
    "        metadata = result.get('metadata', {})\n",
    "        distance = result.get('distance', 0.0)\n",
    "        similarity = 1 - distance\n",
    "        category = metadata.get('category', 'N/A')\n",
    "        print(f\"    {j}. {key} (similarity: {similarity:.3f}, category: {category})\")\n",
    "except Exception as e:\n",
    "    baseline_time = time.time() - baseline_start\n",
    "    print(f\"  ‚ùå Error in baseline search after {baseline_time*1000:.1f}ms: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ Large-scale filtering tests completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d90580e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary and statistics\n",
    "print(\"üìà Performance Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üìä Dataset size: {vectors_inserted:,} vectors\")\n",
    "print(f\"üèóÔ∏è Index type: IVFPQ\")\n",
    "print(f\"üìê Vector dimension: 768\")\n",
    "\n",
    "# Test system metrics\n",
    "print(\"\\n‚öôÔ∏è System Health Check\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Health check endpoint\n",
    "try:\n",
    "    health_response = requests.get(\"http://localhost:8000/health\")\n",
    "    if health_response.status_code == 200:\n",
    "        health_data = health_response.json()\n",
    "        print(f\"‚úÖ API health: {health_data.get('status', 'OK')}\")\n",
    "        print(f\"‚úÖ Implementation: {health_data.get('implementation', 'Unknown')}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Health check returned status: {health_response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Health check failed: {e}\")\n",
    "\n",
    "# Healthz check\n",
    "try:\n",
    "    healthz_response = requests.get(\"http://localhost:8000/healthz\")\n",
    "    if healthz_response.status_code == 200:\n",
    "        healthz_data = healthz_response.json()\n",
    "        print(f\"‚úÖ System health: {healthz_data.get('ok', False)}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Healthz check returned status: {healthz_response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Healthz check failed: {e}\")\n",
    "\n",
    "# Bucket list to confirm our bucket exists\n",
    "try:\n",
    "    buckets_response = s3vectors_client.list_vector_buckets()\n",
    "    buckets = buckets_response.get('vectorBuckets', [])\n",
    "    bucket_names = [b['vectorBucketName'] for b in buckets]\n",
    "    if bucket_name in bucket_names:\n",
    "        print(f\"‚úÖ Target bucket confirmed: {bucket_name}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Target bucket not found in bucket list\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Bucket list check failed: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"üéâ Large-scale test completed successfully!\")\n",
    "print(\"Note: The S3 Vectors API is working well with large datasets\")\n",
    "print(\"      and efficient IVFPQ indexing for semantic search.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}